üéØ Overall Goal

Update the Sentiment Analysis & Topic Modeling page so that:
	1.	It uses only traditional NLP models (no transformers, no LLMs).
	2.	All training and inference happen in the Streamlit app runtime (nothing offline).
	3.	The UI clearly contrasts:
	‚Ä¢	‚ÄúRealtime NLP (traditional)‚Äù ‚Äì fast, local, classical models
	‚Ä¢	‚ÄúLLM‚Äù ‚Äì slower, external, generative (on your LLM page)

VADER and LDA stay as baselines, but we add higher-accuracy classical models that are still fast enough to train and run live.

‚∏ª

1. Sentiment Analysis ‚Äì Realtime Traditional Models

1.1 Keep existing VADER sentiment

No change required:
	‚Ä¢	Keep analyze_sentiment_vader(...) and any existing batch logic.
	‚Ä¢	This is your ‚Äúlexicon-based baseline‚Äù, very fast and fully traditional.

‚∏ª

1.2 Add an in-app trained ML sentiment model (TF-IDF + Linear Classifier)

Key idea:
Train a simple supervised model inside the Streamlit app, using the same dataset the user loads (or a built-in demo dataset). Cache it so training happens only once per session, but it‚Äôs still clearly ‚Äúlive, in runtime.‚Äù

A. Create labels on the fly
Assumption: your demo dataset has a rating-like field (e.g., star_rating or similar).
	1.	When the dataset is loaded in the app (you already do this), create binary labels:

def build_sentiment_labels_from_df(df):
    # Example rule ‚Äì adjust to your schema:
    # rating >= 4 ‚Üí positive (1), rating <= 2 ‚Üí negative (0), drop neutrals
    labeled = df.copy()
    labeled = labeled[labeled["rating"].isin([1, 2, 4, 5])]  # drop 3-star neutrals

    labeled["label"] = labeled["rating"].apply(
        lambda r: 1 if r >= 4 else 0
    )

    texts = labeled["review"].tolist()
    labels = labeled["label"].tolist()
    return texts, labels


	2.	If ratings don‚Äôt exist, devs can:
	‚Ä¢	Use an included labeled dataset, or
	‚Ä¢	Add a small example CSV with a label column.

‚∏ª

B. Train the model in runtime (and cache it)
	1.	Add imports:

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
# or: from sklearn.svm import LinearSVC


	2.	Create a cached training function:

import hashlib
import time
import streamlit as st

@st.cache_resource
def train_ml_sentiment_model(texts, labels):
    # Optional: sample to keep training snappy
    max_samples = 2000
    if len(texts) > max_samples:
        texts = texts[:max_samples]
        labels = labels[:max_samples]

    vectorizer = TfidfVectorizer(
        ngram_range=(1, 2),
        stop_words="english",
        max_features=20000
    )
    X = vectorizer.fit_transform(texts)

    clf = LogisticRegression(max_iter=1000)
    # or LinearSVC(), if you don‚Äôt need probabilities
    clf.fit(X, labels)

    return vectorizer, clf


	3.	When the dataset is loaded:

texts, labels = build_sentiment_labels_from_df(df)
vectorizer, clf = train_ml_sentiment_model(texts, labels)

	‚Ä¢	This training runs once per app session (thanks to @st.cache_resource), but it‚Äôs clearly happening inside the Streamlit runtime.
	‚Ä¢	Training on a subset (e.g., 2k samples) will be very fast on a MacBook.

‚∏ª

C. Add a per-review ML sentiment function

def analyze_sentiment_ml(review_text: str, vectorizer, clf) -> dict:
    start_time = time.time()

    X = vectorizer.transform([review_text])
    pred = clf.predict(X)[0]

    if hasattr(clf, "predict_proba"):
        confidence = float(clf.predict_proba(X)[0].max())
    else:
        confidence = 1.0  # or derive from decision_function if desired

    elapsed_ms = (time.time() - start_time) * 1000

    return {
        "sentiment": "positive" if pred == 1 else "negative",
        "confidence": confidence,
        "elapsed_ms": elapsed_ms,
    }

For batch sentiment (if you show it), just loop over reviews inside the app using this function‚Äîno external batch job.

‚∏ª

D. UI: let users switch models and see speed
On the Sentiment page:

sentiment_model_choice = st.radio(
    "Sentiment model (traditional NLP)",
    ["VADER (lexicon-based)", "TF-IDF + Linear Model (trained live)"]
)

Then, when analyzing a review:

if sentiment_model_choice == "VADER (lexicon-based)":
    result = analyze_sentiment_vader(review_text)
else:
    result = analyze_sentiment_ml(review_text, vectorizer, clf)

st.metric("Sentiment", result["sentiment"])
st.metric("Confidence", f"{result['confidence']:.2f}")
st.metric("Latency (ms)", f"{result['elapsed_ms']:.1f}")

This makes the ‚Äúrealtime NLP vs LLM‚Äù story very clear: both models run live; both are fast; later you compare to the LLM page.

‚∏ª

2. Topic Modeling ‚Äì Realtime Classical Methods

All topic modeling work stays inside Streamlit, too.

2.1 Keep LDA as the baseline
	‚Ä¢	Keep your existing extract_topics_lda(...) implementation.
	‚Ä¢	This is your classical probabilistic topic model baseline.

‚∏ª

2.2 Add NMF (matrix factorization) as an alternative topic model

Goal: Show that a different traditional method (NMF + TF-IDF) can be trained and run in realtime on the same data.
	1.	Add import:

from sklearn.decomposition import NMF


	2.	Implement extract_topics_nmf(reviews, num_topics, words_per_topic):

def extract_topics_nmf(reviews, num_topics: int, words_per_topic: int = 5) -> dict:
    start_time = time.time()

    docs = [r["review"] for r in reviews]

    vectorizer = TfidfVectorizer(
        stop_words="english",
        ngram_range=(1, 2),
        max_features=20000
    )
    X = vectorizer.fit_transform(docs)

    nmf = NMF(n_components=num_topics, random_state=42)
    W = nmf.fit_transform(X)
    H = nmf.components_
    terms = vectorizer.get_feature_names_out()

    topics = []
    for topic_idx, topic_vec in enumerate(H):
        top_indices = topic_vec.argsort()[:-words_per_topic - 1:-1]
        words = [terms[i] for i in top_indices]
        weights = [float(topic_vec[i]) for i in top_indices]

        topics.append({
            "id": topic_idx,
            "words": words,
            "weights": weights,
        })

    elapsed_ms = (time.time() - start_time) * 1000
    return {
        "topics": topics,
        "elapsed_ms": elapsed_ms,
    }



	‚Ä¢	This trains NMF on the fly on the current dataset.
	‚Ä¢	You can optionally cache the result with @st.cache_resource keyed on dataset hash + num_topics.

‚∏ª

2.3 UI: switch between LDA and NMF and show timing

topic_model_choice = st.radio(
    "Topic modeling method (traditional NLP)",
    ["LDA (probabilistic)", "NMF (matrix factorization)"]
)

if topic_model_choice == "LDA (probabilistic)":
    topic_result = extract_topics_lda(reviews, num_topics, words_per_topic)
else:
    topic_result = extract_topics_nmf(reviews, num_topics, words_per_topic)

st.metric("Topic modeling latency (ms)", f"{topic_result['elapsed_ms']:.1f}")
# Then render topics as you already do

Again: all computation is inside the Streamlit runtime, on demand.

‚∏ª

3. Single-Review ‚ÄúTopics‚Äù / Aspects ‚Äì Better Phrases, Still Realtime

3.1 Replace raw word frequency with RAKE/YAKE for a single review

Goal: For one selected review, we want fast, on-the-fly keyphrase extraction that‚Äôs still traditional (no neural models).
	1.	Add dependency and import (example with RAKE):

# pip install rake-nltk
from rake_nltk import Rake


	2.	Update extract_topics_from_single_review(review_text):

def extract_topics_from_single_review(review_text: str) -> dict:
    start_time = time.time()

    rake = Rake()
    rake.extract_keywords_from_text(review_text)
    ranked_phrases = rake.get_ranked_phrases()  # already sorted by importance

    top_phrases = ranked_phrases[:5]

    # Existing logic:
    # - map phrases to aspects (price, quality, shipping, etc.)
    # - run VADER on sentences/phrases per aspect
    aspect_sentiments = compute_aspect_sentiments(review_text, top_phrases)

    elapsed_ms = (time.time() - start_time) * 1000

    return {
        "top_phrases": top_phrases,
        "aspect_sentiments": aspect_sentiments,
        "elapsed_ms": elapsed_ms,
    }


	3.	Show this in the UI for a single selected review, including latency.

All of this is instant on a MacBook and very clearly ‚Äúrealtime NLP.‚Äù

‚∏ª

4. How This Supports the ‚ÄúRealtime NLP vs LLM‚Äù Story

With these changes, devs will enable you to demonstrate:
	‚Ä¢	Traditional Realtime NLP
	‚Ä¢	VADER and TF-IDF + Linear model: trained and run inside the app, sub-second latency.
	‚Ä¢	LDA and NMF topics: trained live on the current dataset, with visible latency metrics.
	‚Ä¢	RAKE/YAKE per-review phrases: realtime extraction and aspect sentiment.
	‚Ä¢	LLM Side (on your other page)
	‚Ä¢	Same text, but:
	‚Ä¢	Sent through an LLM endpoint.
	‚Ä¢	Higher latency and different cost profile.
	‚Ä¢	More ‚Äúintelligent‚Äù generation, but clearly slower / more resource-intensive.

The dev work is mostly:
	‚Ä¢	Adding one runtime-trained sentiment model.
	‚Ä¢	Adding one NMF topic model.
	‚Ä¢	Swapping single-review frequency counts for RAKE.
	‚Ä¢	Adding a few radio buttons + latency metrics.